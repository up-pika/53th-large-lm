# 第一章：引言

## 1.1 什么是语言模型

语言模型（Language Model）的经典定义是一种对令牌序列（Token）的概率分布。
要求：语言模型需要理解人类语言与构词方式，并对符合普遍认知的序列评估准确的概率
语言模型用于做生成任务：即语言模型接受一个序列并返回一个概率来评估其好坏。

语言模型最初是在信息理论的背景下研究的，可用于估计英语的熵

## 1.2 大模型（Large Language Model）的发展

大模型的“大” 体现在模型的参数量越来越大。过去4年中，模型的参数量增加了5000倍：
|Model|Organization|Date|Size (# params)|
|---|---|---|---|
|ELMo|AI2|Feb 2018|94,000,000|
|GPT|OpenAI|Jun 2018|110,000,000|
|BERT|Google|Oct 2018|340,000,000|
|XLM|Facebook|Jan 2019|655,000,000|
|GPT-2|OpenAI|Mar 2019|1,500,000,000|
|RoBERTa|Facebook|Jul 2019|355,000,000|
|Megatron-LM|NVIDIA|Sep 2019|8,300,000,000|
|T5|Google|Oct 2019|11,000,000,000|
|Turing-NLG|Microsoft|Feb 2020|17,000,000,000|
|GPT-3|OpenAI|May 2020|175,000,000,000|
|Megatron-Turing NLG|Microsoft, NVIDIA|Oct 2021|530,000,000,000|
|Gopher|DeepMind|Dec 2021|280,000,000,000|

### 1.2.1 大模型的能力

存在首尾或中间段内容填空形式的prompt格式，这种方式可不断改变提示让模型执行各种各样的任务，比如，机器翻译，文章内容提炼，文章扩写，问答与答题，情感预测等

以 GPT-3 为例，它具有上下文学习的能力。通过构建一个prompt，包含输入/输出，以这种方式可以让模型理解任务，并给出所需答案。

不同于监督学习：监督学习是指定一组输入/输出对的数据集，并训练模型尽可能拟合这些数据，每次训练一般会产生不同参数的模型。但在上下文学习中，只有一个语言模型通过不同的提示完成不同的任务。

### 1.2.3 大模型发展时凸显的风险

1. 可靠性：大模型即使能给出正确的答案，也存在无法解释其中具体的推导说明过程的现象
2. 社会偏见：机器不同于人，在特定的“配料表”预料的喂养下，容易导致模型间的差异化与刻板化，从而产生一些偏见的数据，而作为研究员的我们需要考虑如何训练模型，让其减少偏见
3. 有害性：大模型所学习的预料中难免存在未进行正确言论的清洗过程，或者清洗不到位，还是会产生一些有害性的言论，比如危害社会，不健康心理对应的描述与指导，恐怖与危险的行为建议，又或是不符合人伦的言论
4. 虚假信息：由于大语言模型强大的上下文学习能力与能轻易打破语言学习限制的能力，也容易被恶意操作者用于“弄虚作假”，比如制作虚假信息
5. 安全性：大模型的数据大多是从公网爬取语料数据，公网中极有可能被部署了采集数据的陷阱，没有防范或者不懂防范的公民容易受到数据信息泄露与数据中毒攻击
6. 法律安全：大模型基于版权数据（如书籍）进行参数训练。如何评定这个操作的合法性，如何评定生成内容的版权
7. 成本和环境影响：大模型的训练通常需要数千个GPU的并行化，这是购买配置的成本。供给如此规模的GPU与其他机器的工作，会持续产生碳排放
8. 获取与使用大模型：目前国内存在一些开源可用的大模型API，也存在一些按token收费的API
